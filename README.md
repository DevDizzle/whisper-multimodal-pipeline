# ğŸ™ï¸ Whisper Multimodal Pipeline

**Cross-modal AI: Audio â†’ Text â†’ Intelligence â†’ Structured Output**

A production-ready pipeline that demonstrates multimodal AI capabilities by combining speech-to-text transcription (OpenAI Whisper) with LLM-powered analysis (Google Gemini) to extract structured intelligence from audio content.

---

## Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Input â”‚â”€â”€â”€â–¶â”‚   Transcriber    â”‚â”€â”€â”€â–¶â”‚    Analyzer      â”‚â”€â”€â”€â–¶â”‚ Structured Outputâ”‚
â”‚              â”‚    â”‚                  â”‚    â”‚                  â”‚    â”‚                  â”‚
â”‚ .wav .mp3    â”‚    â”‚ OpenAI Whisper   â”‚    â”‚ Google Gemini    â”‚    â”‚ JSON / Pydantic  â”‚
â”‚ .m4a .flac   â”‚    â”‚ â€” or â€”           â”‚    â”‚                  â”‚    â”‚                  â”‚
â”‚              â”‚    â”‚ GCP Speech-to-   â”‚    â”‚ â€¢ Sentiment      â”‚    â”‚ â€¢ Transcription  â”‚
â”‚              â”‚    â”‚ Text             â”‚    â”‚ â€¢ Entities       â”‚    â”‚ â€¢ Analysis       â”‚
â”‚              â”‚    â”‚                  â”‚    â”‚ â€¢ Topics         â”‚    â”‚ â€¢ Action Items   â”‚
â”‚              â”‚    â”‚ Timestamped      â”‚    â”‚ â€¢ Summary        â”‚    â”‚ â€¢ Confidence     â”‚
â”‚              â”‚    â”‚ Segments         â”‚    â”‚ â€¢ Action Items   â”‚    â”‚   Scores         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

- **Dual transcription backends** â€” OpenAI Whisper (local) or Google Cloud Speech-to-Text (cloud)
- **LLM-powered analysis** â€” Sentiment, named entities, topic classification, summarization, action item extraction via Gemini
- **Structured output** â€” Pydantic-validated JSON results with confidence scores
- **Async pipeline** â€” Full async/await support for concurrent processing
- **Retry logic** â€” Exponential backoff with configurable retry policies
- **Multi-format audio** â€” WAV, MP3, M4A, FLAC, OGG support via pydub/ffmpeg

## ğŸ¯ Use Cases

| Domain | Application |
|--------|-------------|
| **Media & Entertainment** | Automated content analysis for audio/video assets |
| **Customer Experience** | Call center transcript analysis with sentiment tracking |
| **Accessibility** | Real-time captioning with contextual intelligence |
| **Content Production** | Meeting transcription â†’ action items â†’ task assignment |
| **Research** | Interview analysis with entity extraction and topic modeling |

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Transcription | OpenAI Whisper, Google Cloud Speech-to-Text |
| Analysis | Google Gemini 1.5 Pro |
| Data Models | Pydantic v2 |
| Audio Processing | pydub, ffmpeg |
| ML Runtime | PyTorch |
| Async | asyncio, aiofiles |
| Testing | pytest, unittest.mock |
| Config | YAML |

## ğŸš€ Quick Start

```bash
# Clone and install
git clone https://github.com/YOUR_USERNAME/whisper-multimodal-pipeline.git
cd whisper-multimodal-pipeline
pip install -r requirements.txt

# Set your Gemini API key
export GEMINI_API_KEY="your-key-here"

# Run the pipeline
python -m src.pipeline --audio sample_audio/your_clip.wav
```

See [GUIDE.md](GUIDE.md) for detailed setup instructions.

## ğŸ“ Project Structure

```
whisper-multimodal-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transcriber.py      # Whisper/GCP transcription wrapper
â”‚   â”œâ”€â”€ analyzer.py          # Gemini LLM analysis engine
â”‚   â”œâ”€â”€ pipeline.py          # Orchestration with async + retry
â”‚   â””â”€â”€ models.py            # Pydantic data models
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ transcribe_analyze.py  # Full pipeline walkthrough (percent script)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ pipeline_config.yaml   # Model sizes, prompts, settings
â”œâ”€â”€ sample_audio/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ GUIDE.md
â””â”€â”€ README.md
```

## ğŸ“Š Example Output

```json
{
  "transcription": {
    "text": "We need to finalize the character designs by Friday...",
    "segments": [
      {"start": 0.0, "end": 3.2, "text": "We need to finalize the character designs by Friday"}
    ],
    "language": "en",
    "duration_seconds": 45.2
  },
  "analysis": {
    "sentiment": {"label": "neutral-urgent", "score": 0.78},
    "entities": [
      {"text": "Friday", "type": "DATE"},
      {"text": "character designs", "type": "DELIVERABLE"}
    ],
    "topics": ["project management", "creative production", "deadlines"],
    "summary": "Team discussion about finalizing character designs with a Friday deadline.",
    "action_items": [
      {"task": "Finalize character designs", "assignee": null, "deadline": "Friday"}
    ]
  }
}
```

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

*Built to demonstrate cross-modal AI engineering: transforming raw audio into structured, actionable intelligence.*
